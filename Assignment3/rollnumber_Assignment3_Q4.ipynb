{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhSfbBviYpuf"
      },
      "source": [
        "# Assignment 3 ( Question 4) Know the hatred!\n",
        "\n",
        "## Instructions\n",
        "- Run this notebook on ```Google Colab(preferable)```\n",
        "- Write your code and analysis in the indicated cells.\n",
        "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
        "- Do not attempt to change the contents of other cells. \n",
        "\n",
        "## Packages Used\n",
        "- all the packages that are imported in the template code\n",
        "\n",
        "## Submission\n",
        "- Rename the notebook to `<roll_number>_Assignment3_Q4.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmDTRhMldKbU",
        "outputId": "adfba9f5-5239-48f6-c078-e57a9132dfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Vr9cT4n04gsJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import sklearn\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8KX18mePvdv",
        "outputId": "fd734517-418e-4572-9177-04d786fb9c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqr3rUr4UNzh"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem.\n",
        "\n",
        "The dataset here is a multi-label classification dataset. To understand multi-label classification datasets, you can refer here: https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/ \n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "Here, we will be using Binary relevance to solve our multi-label classification problem\n",
        "\n",
        "\n",
        "\n",
        "**Binary Relevance**: This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n",
        "\n",
        "Finally, we will be summing up the accuracies obtained of all labels to obtain the final accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "Xsp3trnR8gLH",
        "outputId": "c0114efa-5349-4275-b64b-1d6bc2a00123"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "111598  5508f663f3fa9098  \"\\n\\n Your help desk request \\n\\nI've replied ...   \n",
              "8464    167f69436e467005  Furthermore Note 1 says it all (Housemates wer...   \n",
              "115034  671e0c56ec3e5dc6  I documented the change I made on Talk. The 'o...   \n",
              "103628  2a7038eabab79523  External links\\nI've removed the list of exter...   \n",
              "145929  24566d3ba647e7a2  When are you going to learn that a definition ...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
              "111598      0             0        0       0       0              0  \n",
              "8464        0             0        0       0       0              0  \n",
              "115034      0             0        0       0       0              0  \n",
              "103628      0             0        0       0       0              0  \n",
              "145929      1             0        0       0       0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90413f20-f3cb-43b2-9e8e-a97ce876a04e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111598</th>\n",
              "      <td>5508f663f3fa9098</td>\n",
              "      <td>\"\\n\\n Your help desk request \\n\\nI've replied ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8464</th>\n",
              "      <td>167f69436e467005</td>\n",
              "      <td>Furthermore Note 1 says it all (Housemates wer...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115034</th>\n",
              "      <td>671e0c56ec3e5dc6</td>\n",
              "      <td>I documented the change I made on Talk. The 'o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103628</th>\n",
              "      <td>2a7038eabab79523</td>\n",
              "      <td>External links\\nI've removed the list of exter...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145929</th>\n",
              "      <td>24566d3ba647e7a2</td>\n",
              "      <td>When are you going to learn that a definition ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90413f20-f3cb-43b2-9e8e-a97ce876a04e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90413f20-f3cb-43b2-9e8e-a97ce876a04e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90413f20-f3cb-43b2-9e8e-a97ce876a04e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# loading the dataset\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/q4/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/q4/test.csv')\n",
        "labels_df = pd.read_csv('/content/drive/MyDrive/q4/test_labels.csv')\n",
        "test_df = pd.merge(test_df, labels_df,on='id')\n",
        "index_names = test_df[ (test_df['toxic'] == -1) & (test_df['severe_toxic'] == -1) & (test_df['severe_toxic'] == -1) & (test_df['obscene'] == -1) & (test_df['threat'] == -1)].index\n",
        "test_df = test_df.drop(index_names,inplace=False)\n",
        "#dataset understanding\n",
        "train_df.sample(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rY86XM-xm51A"
      },
      "outputs": [],
      "source": [
        "y_test_val = test_df[['toxic',\t'severe_toxic',\t'obscene',\t'threat',\t'insult',\t'identity_hate']].values\n",
        "y_train_val = train_df[['toxic',\t'severe_toxic',\t'obscene',\t'threat',\t'insult',\t'identity_hate']].values\n",
        "test_df = test_df[['id','comment_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di6n18tL7kc2",
        "outputId": "c1c2d23d-1d50-4e0f-b80c-665ebc478786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in test is 63978\n",
            "Total rows in train is 159571\n"
          ]
        }
      ],
      "source": [
        "# let's see the total rows in train, test data and the numbers for the various categories\n",
        "print('Total rows in test is {}'.format(len(test_df)))\n",
        "print('Total rows in train is {}'.format(len(train_df)))\n",
        "#print(train_df[cols_target].sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ciejlBb074r3"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bknzbJYX8CEN"
      },
      "outputs": [],
      "source": [
        "train_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))\n",
        "test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ozooblAOQBG",
        "outputId": "efe3209d-befb-4502-ad8b-b23f39824707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(159571,) (63978,)\n"
          ]
        }
      ],
      "source": [
        "# train_df = train_df.drop('char_length',axis=1)\n",
        "X = train_df.comment_text\n",
        "test_X = test_df.comment_text\n",
        "print(X.shape, test_X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEPiqW7nOeZM"
      },
      "source": [
        "Now, we will be experimenting this classification problem with different classifiers. We will be using pre-trained models inorder to obtain the required embeddings. Here, we will be using BERT and Universal Sentence Encoder embeddings.\n",
        "\n",
        "\n",
        "Understand how Elmo and USE(Universal Sentence Encoder) works here. You can use any of both to train your classifiers.\n",
        "\n",
        "To understand how different sentence embeddings work, you can refer to this link: https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial \n",
        "\n",
        "Please give it a look before you start working with transfer learning/ sentence embeddings like BERT, Elmo, Ulm-fit, USE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "C_YhipFSzNaC"
      },
      "outputs": [],
      "source": [
        "#loading universal sentence encoder\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# Load pre-trained universal sentence encoder model\n",
        "use_embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "# elmo_embed = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-LoX3HCa94G7"
      },
      "outputs": [],
      "source": [
        "def get_use_embedding(sentence):\n",
        "  embedding = use_embed([sentence])\n",
        "  embedding = embedding.numpy()\n",
        "  return embedding[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uCTw6H_sPg_m"
      },
      "outputs": [],
      "source": [
        "#to obtain bert embeddings ( you need not completely understand how it works)\n",
        "#you can directly use the function get_bert_embedding to obtain the embedding\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "\n",
        "def get_bert_embedding(sentence):\n",
        "  encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "  sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "  return sentence_embedding[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "A3eafXPxQYp4"
      },
      "outputs": [],
      "source": [
        "#Obtain X_test and X_train \n",
        "bert_x_test = np.loadtxt('/content/drive/MyDrive/test_bert.txt')\n",
        "bert_x_train = np.loadtxt('/content/drive/MyDrive/train_bert.txt')\n",
        "use_x_train = np.loadtxt('/content/drive/MyDrive/train_use.txt')\n",
        "use_x_test = np.loadtxt('/content/drive/MyDrive/test_use.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY4InEH5__Ea"
      },
      "source": [
        "# Classifiers\n",
        "Here, we use different classifiers for our classification problem. You can directly import classifiers for first two.\n",
        "\n",
        "\n",
        "*   Naive Bayes classifier\n",
        "*   Support Vector Machine\n",
        "*   Multi-layer Perceptron(MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygV8mXTOrVDo",
        "outputId": "bc0e88a9-648e-4ab4-8628-cf91e07b2d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Naive Bayes classifier on USE embedding"
      ],
      "metadata": {
        "id": "Ns5N4Jb3qCPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS2_lOBIzM8u",
        "outputId": "555a1d11-726a-4aa5-8664-8779bbec5e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7983838194379318\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "classifier = BinaryRelevance(GaussianNB())\n",
        "classifier.fit(use_x_train, y_train_val)\n",
        "predictions = classifier.predict(use_x_test)\n",
        "print('Accuracy:', accuracy_score(y_test_val, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Naive Bayes classifier on BERT embedding"
      ],
      "metadata": {
        "id": "NJPXwMPQp_hi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kh10hizr26C",
        "outputId": "49453825-ca72-4fdf-8989-d6784b11b46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7384413392103536\n"
          ]
        }
      ],
      "source": [
        "\n",
        "classifier = BinaryRelevance(GaussianNB())\n",
        "classifier.fit(bert_x_train, y_train_val)\n",
        "predictions = classifier.predict(bert_x_test)\n",
        "print('Accuracy:', accuracy_score(y_test_val, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Support Vector Machines on USE embedding"
      ],
      "metadata": {
        "id": "Ijxc8pL6p6M_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ_02S_RzM6f",
        "outputId": "e47d6b87-32ea-47a6-be36-ade533061246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8930257275938603\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = BinaryRelevance(LinearSVC(dual=False))\n",
        "classifier.fit(use_x_train, y_train_val)\n",
        "predictions = classifier.predict(use_x_test)\n",
        "print('Accuracy:', accuracy_score(y_test_val, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Support Vector Machines on BERT embedding"
      ],
      "metadata": {
        "id": "OHgPvdLXp0q7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTvWeGkYsjWZ",
        "outputId": "df9195d2-c960-43c6-8d2b-eebb876f6e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8893994810716184\n"
          ]
        }
      ],
      "source": [
        "\n",
        "classifier = BinaryRelevance(LinearSVC(dual=False))\n",
        "classifier.fit(bert_x_train, y_train_val)\n",
        "predictions = classifier.predict(bert_x_test)\n",
        "print('Accuracy:', accuracy_score(y_test_val, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHOBzVA7Cg35"
      },
      "source": [
        "Which classifier of the three performed better, and why do you think so ?\n",
        "\n",
        "Ans:\n",
        "In terms of accuracy, SVM outperformed the other two models. The essential premise of Naive Bayes is that all of the characteristics are independent of one another. SVM, on the other hand, seeks to identify some sort of relationship between features. \n",
        " With strategies like One vs Rest and One vs One, the SVM may be employed and works quite well in Multilabel classification. The MLP, on the other hand, is ideal for singleclass categorization. For sklearn MLP,  performance will improve if different activations cannot be used on different layers. This allowed the SVM  to achieve slightly higher accuracy than the sklearn MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsWLvpyCqya"
      },
      "source": [
        "#Multi-layer Perceptron\n",
        "\n",
        "For Multi-layer Perceptron, first you will be implementing your own articial Neural network, and then compare it with sklearn's MLP layer.\n",
        "\n",
        "\n",
        "Implement your own artificial Neural Network (MLP)\n",
        "Few Steps to be followed(for your reference):\n",
        "\n",
        "*   Initialise the weights randomly\n",
        "*   Decide upon the number of layers you wanna have and number of neurons associated in each layer. ( Have a clear idea of what will be the size of the weight matrices).\n",
        "*   Write the code for forward and backward propogations\n",
        "*   Decide upon the loss function\n",
        "*   Train the model\n",
        "*   Predict the labels after training the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP Model on USE Embedding\n",
        "\n",
        "x_list = list()\n",
        "for i in range(0, 6, 1):\n",
        "  x_list.append(y_train_val.T[i].reshape(len(y_train_val), -1))\n"
      ],
      "metadata": {
        "id": "H-Wm6X8bP2ks"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can remove returns statements if u think they are not required\n",
        "# you can also add functions if needed\n",
        "\n",
        "\n",
        "def initialise_weights(input_shape):\n",
        "  loc_val = 0.0\n",
        "  scale_val = 0.01\n",
        "  input_size = 100\n",
        "  w1 = np.random.normal(loc=loc_val, scale = scale_val, size = (input_shape,input_size))\n",
        "  w2 = np.random.normal(loc=loc_val, scale = scale_val, size = (input_size,1))\n",
        "  return w1, w2\n",
        "  \n",
        "\n",
        "def loss_function(y_dash, y):\n",
        "  # mse = np.mean((y_dash - y)**2,axis=0)\n",
        "  # g_loss = (y_dash - y)\n",
        "  return np.mean(np.square(y_dash - y),axis=0), (y_dash - y)\n",
        "\n",
        "\n",
        "def sig_fn(x):\n",
        "  \"\"\"\n",
        "    This is Sigmoid function\n",
        "  \"\"\"\n",
        "  return (1 / (1 + np.exp((-1) * x)))\n",
        "\n",
        "\n",
        "def diff_sig_fn(x):\n",
        "  \"\"\"\n",
        "    This is derivative of sigmoid function\n",
        "  \"\"\"\n",
        "  return np.multiply(sig_fn(x), 1 - sig_fn(x))\n",
        "\n",
        "\n",
        "def forward_layer(w1, w2, X):\n",
        "  \"\"\"\n",
        "    This Fn calculates output of the MLP using weights.\n",
        "  \"\"\"\n",
        "  z1 = np.dot(X, w1)\n",
        "  z2 = sig_fn(z1)\n",
        "  y1 = np.dot(z2, w2)\n",
        "  y2 = sig_fn(y1)\n",
        "  return z2, y2\n",
        "\n",
        "\n",
        "def backward_propogation(out1, dsig1, dsig2, g_loss, W2, X):\n",
        "  \"\"\"\n",
        "    This Fn train the weights of MLP using its output value.\n",
        "  \"\"\"\n",
        "  del_w2 = np.dot(out1.T , np.multiply(dsig2, g_loss))\n",
        "  del_g = np.dot(np.multiply(dsig2, g_loss), W2.T)\n",
        "  del_w1 = np.dot(X.T, np.multiply(dsig1, del_g))\n",
        "  return del_w1, del_w2"
      ],
      "metadata": {
        "id": "dbeCp7MDP-PG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x_train, y_train):\n",
        "  \"\"\"\n",
        "  This function trains the MLP and returns weights.\n",
        "\n",
        "  @Return:\n",
        "    Weights: tuple()\n",
        "  \"\"\"\n",
        "\n",
        "    W1, W2 = initialise_weights(x_train.shape[1])\n",
        "\n",
        "    lr = 0.07\n",
        "    epochs = 25\n",
        "    n = len(x_train)\n",
        "    batch_size = 100\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        \n",
        "        loss_list = []\n",
        "        for j in range(0, n, batch_size):\n",
        "            X = x_train[j:j+batch_size]\n",
        "            Y = y_train[j:j+batch_size]\n",
        "            out1, out2 = forward_layer(W1, W2, X)\n",
        "\n",
        "            loss, g_loss = loss_function(out2, Y)\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            dsig_1 = diff_sig_fn(out1)\n",
        "            dsig_2 = diff_sig_fn(out2)\n",
        "\n",
        "            del_w1, del_w2 = backward_propogation(out1, dsig_1, dsig_2, g_loss, W2, X)\n",
        "            W1 = W1 - lr*del_w1\n",
        "            W2 = W2 - lr*del_w2\n",
        "        print(f\"[ {i+1} / {epochs} ] loss: {np.mean(loss_list)}\")\n",
        "        # print(\"Loss: \", np.mean(loss_list))\n",
        "    return W1, W2\n",
        "\n",
        "\n",
        "def assign_class(y):\n",
        "    for i in range(len(y)):\n",
        "      if y[i] < 0.5:\n",
        "        y[i] = 0\n",
        "      else:\n",
        "        y[i] = 1\n",
        "    return y\n",
        "\n",
        "def predict(w1, w2, x_test):\n",
        "    z = sig_fn(np.dot(x_test, w1))\n",
        "    y = sig_fn(np.dot(z, w2))\n",
        "    return assign_class(y)\n",
        "    # return y"
      ],
      "metadata": {
        "id": "yPq8ZjDwQHOr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Model on USE Embedding"
      ],
      "metadata": {
        "id": "YlCdUZysoKP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"For Label 1:\\n\")\n",
        "l1_w1, l1_w2 = train(use_x_train, x_list[0])\n",
        "l1_y_pred = predict(l1_w1, l1_w2, use_x_test)\n",
        "\n",
        "print(\"For Label 2:\\n\")\n",
        "l2_w1, l2_w2 = train(use_x_train, x_list[1])\n",
        "l2_y_pred = predict(l2_w1, l2_w2, use_x_test)\n",
        "\n",
        "print(\"For Label 3:\\n\")\n",
        "l3_w1, l3_w2 = train(use_x_train, x_list[2])\n",
        "l3_y_pred = predict(l3_w1, l3_w2, use_x_test)\n",
        "\n",
        "print(\"For Label 4:\\n\")\n",
        "l4_w1, l4_w2 = train(use_x_train, x_list[3])\n",
        "l4_y_pred = predict(l4_w1, l4_w2, use_x_test)\n",
        "\n",
        "print(\"For Label 5:\\n\")\n",
        "l5_w1, l5_w2 = train(use_x_train, x_list[4])\n",
        "l5_y_pred = predict(l5_w1, l5_w2, use_x_test)\n",
        "\n",
        "print(\"For Label 6:\\n\")\n",
        "l6_w1, l6_w2 = train(use_x_train, x_list[5])\n",
        "l6_y_pred = predict(l6_w1, l6_w2, use_x_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLsTGVh7QN7T",
        "outputId": "371fabe2-4ed4-4d00-f7a3-74d3d4b5b8db"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Label 1:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.05888850998476255\n",
            "[ 2 / 25 ] loss: 0.03818389502707432\n",
            "[ 3 / 25 ] loss: 0.03761992230448517\n",
            "[ 4 / 25 ] loss: 0.03731822130408709\n",
            "[ 5 / 25 ] loss: 0.03710690231496893\n",
            "[ 6 / 25 ] loss: 0.03694444430375018\n",
            "[ 7 / 25 ] loss: 0.03681294980524407\n",
            "[ 8 / 25 ] loss: 0.036702803461887284\n",
            "[ 9 / 25 ] loss: 0.03660823106258714\n",
            "[ 10 / 25 ] loss: 0.03652550967831609\n",
            "[ 11 / 25 ] loss: 0.03645211181840317\n",
            "[ 12 / 25 ] loss: 0.036386250352633355\n",
            "[ 13 / 25 ] loss: 0.036326618173087036\n",
            "[ 14 / 25 ] loss: 0.03627223115588284\n",
            "[ 15 / 25 ] loss: 0.03622232950739792\n",
            "[ 16 / 25 ] loss: 0.036176313689922815\n",
            "[ 17 / 25 ] loss: 0.03613370158229061\n",
            "[ 18 / 25 ] loss: 0.03609409907664409\n",
            "[ 19 / 25 ] loss: 0.03605717940346743\n",
            "[ 20 / 25 ] loss: 0.03602266826528095\n",
            "[ 21 / 25 ] loss: 0.03599033292456513\n",
            "[ 22 / 25 ] loss: 0.0359599740415934\n",
            "[ 23 / 25 ] loss: 0.03593141946339167\n",
            "[ 24 / 25 ] loss: 0.03590451942323612\n",
            "[ 25 / 25 ] loss: 0.035879142777734296\n",
            "For Label 2:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.010029751350825879\n",
            "[ 2 / 25 ] loss: 0.008461398249318344\n",
            "[ 3 / 25 ] loss: 0.007433968910803277\n",
            "[ 4 / 25 ] loss: 0.0072808487478976955\n",
            "[ 5 / 25 ] loss: 0.00718595782060734\n",
            "[ 6 / 25 ] loss: 0.007130569954529388\n",
            "[ 7 / 25 ] loss: 0.007094607235710224\n",
            "[ 8 / 25 ] loss: 0.007066948118534222\n",
            "[ 9 / 25 ] loss: 0.007043467478325238\n",
            "[ 10 / 25 ] loss: 0.007022744494901599\n",
            "[ 11 / 25 ] loss: 0.007004166715055554\n",
            "[ 12 / 25 ] loss: 0.006987365929124515\n",
            "[ 13 / 25 ] loss: 0.0069720664985805725\n",
            "[ 14 / 25 ] loss: 0.006958043882299945\n",
            "[ 15 / 25 ] loss: 0.0069451113524708645\n",
            "[ 16 / 25 ] loss: 0.006933113608156094\n",
            "[ 17 / 25 ] loss: 0.006921922008128819\n",
            "[ 18 / 25 ] loss: 0.006911430313132203\n",
            "[ 19 / 25 ] loss: 0.00690155083052299\n",
            "[ 20 / 25 ] loss: 0.006892211024663725\n",
            "[ 21 / 25 ] loss: 0.006883350632912619\n",
            "[ 22 / 25 ] loss: 0.006874919278596476\n",
            "[ 23 / 25 ] loss: 0.0068668745372969355\n",
            "[ 24 / 25 ] loss: 0.006859180394864725\n",
            "[ 25 / 25 ] loss: 0.006851806030668343\n",
            "For Label 3:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.034852758491910205\n",
            "[ 2 / 25 ] loss: 0.021353353389644038\n",
            "[ 3 / 25 ] loss: 0.020805685912317688\n",
            "[ 4 / 25 ] loss: 0.0205906466247332\n",
            "[ 5 / 25 ] loss: 0.020459313203170622\n",
            "[ 6 / 25 ] loss: 0.020365456068962343\n",
            "[ 7 / 25 ] loss: 0.020293120799181566\n",
            "[ 8 / 25 ] loss: 0.020234753920278133\n",
            "[ 9 / 25 ] loss: 0.020186105632427735\n",
            "[ 10 / 25 ] loss: 0.020144536709375268\n",
            "[ 11 / 25 ] loss: 0.020108302400139538\n",
            "[ 12 / 25 ] loss: 0.02007619737922943\n",
            "[ 13 / 25 ] loss: 0.020047360643924653\n",
            "[ 14 / 25 ] loss: 0.02002116054167432\n",
            "[ 15 / 25 ] loss: 0.01999712342419221\n",
            "[ 16 / 25 ] loss: 0.019974887520663193\n",
            "[ 17 / 25 ] loss: 0.019954172090976482\n",
            "[ 18 / 25 ] loss: 0.019934756207549746\n",
            "[ 19 / 25 ] loss: 0.019916463811675725\n",
            "[ 20 / 25 ] loss: 0.019899152977805184\n",
            "[ 21 / 25 ] loss: 0.019882708069503117\n",
            "[ 22 / 25 ] loss: 0.01986703392401837\n",
            "[ 23 / 25 ] loss: 0.019852051485256202\n",
            "[ 24 / 25 ] loss: 0.019837694486717548\n",
            "[ 25 / 25 ] loss: 0.01982390690579919\n",
            "For Label 4:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.0031376378862328956\n",
            "[ 2 / 25 ] loss: 0.0029846914479439535\n",
            "[ 3 / 25 ] loss: 0.002981130046112895\n",
            "[ 4 / 25 ] loss: 0.0029728633986906994\n",
            "[ 5 / 25 ] loss: 0.0029277592553513393\n",
            "[ 6 / 25 ] loss: 0.0026998795419863842\n",
            "[ 7 / 25 ] loss: 0.002383874741826649\n",
            "[ 8 / 25 ] loss: 0.0022017413683866646\n",
            "[ 9 / 25 ] loss: 0.002120954299277743\n",
            "[ 10 / 25 ] loss: 0.002081921749231796\n",
            "[ 11 / 25 ] loss: 0.002057465565978757\n",
            "[ 12 / 25 ] loss: 0.00203895731935186\n",
            "[ 13 / 25 ] loss: 0.0020236962387656437\n",
            "[ 14 / 25 ] loss: 0.0020106618626364934\n",
            "[ 15 / 25 ] loss: 0.001999327848674113\n",
            "[ 16 / 25 ] loss: 0.0019893433562531105\n",
            "[ 17 / 25 ] loss: 0.0019804454943955422\n",
            "[ 18 / 25 ] loss: 0.001972430576622397\n",
            "[ 19 / 25 ] loss: 0.00196513975706213\n",
            "[ 20 / 25 ] loss: 0.0019584487350718955\n",
            "[ 21 / 25 ] loss: 0.001952259572749744\n",
            "[ 22 / 25 ] loss: 0.00194649428862918\n",
            "[ 23 / 25 ] loss: 0.0019410900128615921\n",
            "[ 24 / 25 ] loss: 0.0019359954203622323\n",
            "[ 25 / 25 ] loss: 0.0019311681409829426\n",
            "For Label 5:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.034854120440362145\n",
            "[ 2 / 25 ] loss: 0.02332560318587522\n",
            "[ 3 / 25 ] loss: 0.02289712353666402\n",
            "[ 4 / 25 ] loss: 0.022721910920896496\n",
            "[ 5 / 25 ] loss: 0.02261272038825652\n",
            "[ 6 / 25 ] loss: 0.022533076513027443\n",
            "[ 7 / 25 ] loss: 0.022470452150211803\n",
            "[ 8 / 25 ] loss: 0.02241893174261963\n",
            "[ 9 / 25 ] loss: 0.022375177617278717\n",
            "[ 10 / 25 ] loss: 0.022337111926452637\n",
            "[ 11 / 25 ] loss: 0.02230336193519099\n",
            "[ 12 / 25 ] loss: 0.02227298227482296\n",
            "[ 13 / 25 ] loss: 0.022245300067845142\n",
            "[ 14 / 25 ] loss: 0.02221982274683382\n",
            "[ 15 / 25 ] loss: 0.022196180786522295\n",
            "[ 16 / 25 ] loss: 0.022174090951779482\n",
            "[ 17 / 25 ] loss: 0.022153332042340535\n",
            "[ 18 / 25 ] loss: 0.02213372846448336\n",
            "[ 19 / 25 ] loss: 0.022115138827442465\n",
            "[ 20 / 25 ] loss: 0.02209744784298261\n",
            "[ 21 / 25 ] loss: 0.02208056044768476\n",
            "[ 22 / 25 ] loss: 0.022064397455475154\n",
            "[ 23 / 25 ] loss: 0.02204889228700546\n",
            "[ 24 / 25 ] loss: 0.02203398847252223\n",
            "[ 25 / 25 ] loss: 0.02201963772081906\n",
            "For Label 6:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.00888515830508165\n",
            "[ 2 / 25 ] loss: 0.00835235455966224\n",
            "[ 3 / 25 ] loss: 0.0066871316406259835\n",
            "[ 4 / 25 ] loss: 0.006183333835299017\n",
            "[ 5 / 25 ] loss: 0.006055960347347794\n",
            "[ 6 / 25 ] loss: 0.005981986639592213\n",
            "[ 7 / 25 ] loss: 0.005930956495339021\n",
            "[ 8 / 25 ] loss: 0.0058926127278495005\n",
            "[ 9 / 25 ] loss: 0.0058617753793384735\n",
            "[ 10 / 25 ] loss: 0.005835803406996225\n",
            "[ 11 / 25 ] loss: 0.0058132690015284885\n",
            "[ 12 / 25 ] loss: 0.005793319118714712\n",
            "[ 13 / 25 ] loss: 0.005775392337464678\n",
            "[ 14 / 25 ] loss: 0.005759093519618084\n",
            "[ 15 / 25 ] loss: 0.0057441333047976145\n",
            "[ 16 / 25 ] loss: 0.005730294521880613\n",
            "[ 17 / 25 ] loss: 0.005717411130922417\n",
            "[ 18 / 25 ] loss: 0.005705354064683945\n",
            "[ 19 / 25 ] loss: 0.005694021396071728\n",
            "[ 20 / 25 ] loss: 0.005683331396662996\n",
            "[ 21 / 25 ] loss: 0.005673217575111818\n",
            "[ 22 / 25 ] loss: 0.005663625086434488\n",
            "[ 23 / 25 ] loss: 0.0056545080995796275\n",
            "[ 24 / 25 ] loss: 0.005645827842431264\n",
            "[ 25 / 25 ] loss: 0.005637551131825415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#report accuracies on the test dataset \n",
        "result = np.concatenate((l1_y_pred, l2_y_pred, l3_y_pred, l4_y_pred, l5_y_pred, l6_y_pred), axis=1)\n",
        "acc = accuracy_score(result, y_test_val)\n",
        "print(\"Accuracy :\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuECUXFGQTIi",
        "outputId": "ce6341b2-6d81-4507-b7dd-6262d51902ac"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.899887461314827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Model with BERT Embedding"
      ],
      "metadata": {
        "id": "nLb_lDu8pfnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_list_bert = list()\n",
        "for i in range(0, 6, 1):\n",
        "  x_list_bert.append(y_train_val.T[i].reshape(len(y_train_val), -1))"
      ],
      "metadata": {
        "id": "eUdb4qMOQXgJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using MLP on BERT embedding\n",
        "\n",
        "print(\"For Label 1:\\n\")\n",
        "l1_w1_bert, l1_w2_bert = train(bert_x_train, x_list_bert[0])\n",
        "l1_y_pred_bert = predict(l1_w1_bert, l1_w2_bert, bert_x_test)\n",
        "\n",
        "\n",
        "print(\"For Label 2:\\n\")\n",
        "l2_w1_bert, l2_w2_bert = train(bert_x_train, x_list_bert[1])\n",
        "l2_y_pred_bert = predict(l2_w1_bert, l2_w2_bert, bert_x_test)\n",
        "\n",
        "\n",
        "print(\"For Label 3:\\n\")\n",
        "l3_w1_bert, l3_w2_bert = train(bert_x_train, x_list_bert[2])\n",
        "l3_y_pred_bert = predict(l3_w1_bert, l3_w2_bert, bert_x_test)\n",
        "\n",
        "\n",
        "print(\"For Label 4:\\n\")\n",
        "l4_w1_bert, l4_w2_bert = train(bert_x_train, x_list_bert[3])\n",
        "l4_y_pred_bert = predict(l4_w1_bert, l4_w2_bert, bert_x_test)\n",
        "\n",
        "\n",
        "print(\"For Label 5:\\n\")\n",
        "l5_w1_bert, l5_w2_bert = train(bert_x_train, x_list_bert[4])\n",
        "l5_y_pred_bert = predict(l5_w1_bert, l5_w2_bert, bert_x_test)\n",
        "\n",
        "\n",
        "print(\"For Label 6:\\n\")\n",
        "l6_w1_bert, l6_w2_bert = train(bert_x_train, x_list_bert[5])\n",
        "l6_y_pred_bert = predict(l6_w1_bert, l6_w2_bert, bert_x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mB0gRAaQc6S",
        "outputId": "ad81cfb2-c924-41b7-f12f-9bb5f160fbdb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Label 1:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.055512617638638045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2 / 25 ] loss: 0.05582753834301355\n",
            "[ 3 / 25 ] loss: 0.055860497766532353\n",
            "[ 4 / 25 ] loss: 0.055869359291767444\n",
            "[ 5 / 25 ] loss: 0.05586893894391055\n",
            "[ 6 / 25 ] loss: 0.05587899355555013\n",
            "[ 7 / 25 ] loss: 0.05589134821022003\n",
            "[ 8 / 25 ] loss: 0.05591173395911775\n",
            "[ 9 / 25 ] loss: 0.05589400752469836\n",
            "[ 10 / 25 ] loss: 0.05590745431395836\n",
            "[ 11 / 25 ] loss: 0.05590340467932812\n",
            "[ 12 / 25 ] loss: 0.05591464794927425\n",
            "[ 13 / 25 ] loss: 0.05591616388180174\n",
            "[ 14 / 25 ] loss: 0.05592254799454017\n",
            "[ 15 / 25 ] loss: 0.055917208665833086\n",
            "[ 16 / 25 ] loss: 0.05590963326696509\n",
            "[ 17 / 25 ] loss: 0.05591560858014584\n",
            "[ 18 / 25 ] loss: 0.055913763318892584\n",
            "[ 19 / 25 ] loss: 0.05590537767346804\n",
            "[ 20 / 25 ] loss: 0.0559147808101102\n",
            "[ 21 / 25 ] loss: 0.05591438121875329\n",
            "[ 22 / 25 ] loss: 0.055908398115219665\n",
            "[ 23 / 25 ] loss: 0.05591327180080136\n",
            "[ 24 / 25 ] loss: 0.05591063655558854\n",
            "[ 25 / 25 ] loss: 0.05590892673770055\n",
            "For Label 2:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.00758343717622294\n",
            "[ 2 / 25 ] loss: 0.0070886556858579365\n",
            "[ 3 / 25 ] loss: 0.006995493320394897\n",
            "[ 4 / 25 ] loss: 0.006949069934368059\n",
            "[ 5 / 25 ] loss: 0.006918620402950371\n",
            "[ 6 / 25 ] loss: 0.006895937346404992\n",
            "[ 7 / 25 ] loss: 0.00687791771602745\n",
            "[ 8 / 25 ] loss: 0.006862999404071258\n",
            "[ 9 / 25 ] loss: 0.006850278692580019\n",
            "[ 10 / 25 ] loss: 0.0068391896092613634\n",
            "[ 11 / 25 ] loss: 0.006829359778005381\n",
            "[ 12 / 25 ] loss: 0.00682053609908956\n",
            "[ 13 / 25 ] loss: 0.00681254213085335\n",
            "[ 14 / 25 ] loss: 0.006805251560606782\n",
            "[ 15 / 25 ] loss: 0.006798570801761633\n",
            "[ 16 / 25 ] loss: 0.006792427316404914\n",
            "[ 17 / 25 ] loss: 0.006786761779146437\n",
            "[ 18 / 25 ] loss: 0.006781522962881324\n",
            "[ 19 / 25 ] loss: 0.006776664733497984\n",
            "[ 20 / 25 ] loss: 0.00677214478603144\n",
            "[ 21 / 25 ] loss: 0.006767924649745451\n",
            "[ 22 / 25 ] loss: 0.0067639702436405585\n",
            "[ 23 / 25 ] loss: 0.0067602522764004915\n",
            "[ 24 / 25 ] loss: 0.00675674615789768\n",
            "[ 25 / 25 ] loss: 0.006753431513465281\n",
            "For Label 3:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.03131923543039127\n",
            "[ 2 / 25 ] loss: 0.0313698341417023\n",
            "[ 3 / 25 ] loss: 0.03153676373441181\n",
            "[ 4 / 25 ] loss: 0.031577645585584874\n",
            "[ 5 / 25 ] loss: 0.03160541292556319\n",
            "[ 6 / 25 ] loss: 0.031616558214004546\n",
            "[ 7 / 25 ] loss: 0.03162529076805693\n",
            "[ 8 / 25 ] loss: 0.03163198478886911\n",
            "[ 9 / 25 ] loss: 0.031635007444598545\n",
            "[ 10 / 25 ] loss: 0.031636439808279564\n",
            "[ 11 / 25 ] loss: 0.03163877192922233\n",
            "[ 12 / 25 ] loss: 0.03164127433248689\n",
            "[ 13 / 25 ] loss: 0.031649833406165194\n",
            "[ 14 / 25 ] loss: 0.031650413736813716\n",
            "[ 15 / 25 ] loss: 0.03165683122713283\n",
            "[ 16 / 25 ] loss: 0.031661928827703384\n",
            "[ 17 / 25 ] loss: 0.031666035613109954\n",
            "[ 18 / 25 ] loss: 0.031667121994685896\n",
            "[ 19 / 25 ] loss: 0.031664695461925095\n",
            "[ 20 / 25 ] loss: 0.031663735806585776\n",
            "[ 21 / 25 ] loss: 0.03166703023270163\n",
            "[ 22 / 25 ] loss: 0.03166459252260081\n",
            "[ 23 / 25 ] loss: 0.03166253512407043\n",
            "[ 24 / 25 ] loss: 0.03166485045816981\n",
            "[ 25 / 25 ] loss: 0.031666239263001834\n",
            "For Label 4:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.0024176270672242547\n",
            "[ 2 / 25 ] loss: 0.002086494134934954\n",
            "[ 3 / 25 ] loss: 0.0020399024775583537\n",
            "[ 4 / 25 ] loss: 0.00201334743773098\n",
            "[ 5 / 25 ] loss: 0.001997659069866714\n",
            "[ 6 / 25 ] loss: 0.0019878623959766893\n",
            "[ 7 / 25 ] loss: 0.001980732638823561\n",
            "[ 8 / 25 ] loss: 0.0019749909812174205\n",
            "[ 9 / 25 ] loss: 0.0019701442097849503\n",
            "[ 10 / 25 ] loss: 0.001965948536100851\n",
            "[ 11 / 25 ] loss: 0.0019622682454770836\n",
            "[ 12 / 25 ] loss: 0.0019590447328492267\n",
            "[ 13 / 25 ] loss: 0.001956294605308931\n",
            "[ 14 / 25 ] loss: 0.0019540785505758667\n",
            "[ 15 / 25 ] loss: 0.0019524287834209396\n",
            "[ 16 / 25 ] loss: 0.0019512975463375707\n",
            "[ 17 / 25 ] loss: 0.0019505721418293259\n",
            "[ 18 / 25 ] loss: 0.0019501273208729613\n",
            "[ 19 / 25 ] loss: 0.001949863514940295\n",
            "[ 20 / 25 ] loss: 0.0019497141946835782\n",
            "[ 21 / 25 ] loss: 0.0019496364387419096\n",
            "[ 22 / 25 ] loss: 0.001949600259484289\n",
            "[ 23 / 25 ] loss: 0.0019495812135117362\n",
            "[ 24 / 25 ] loss: 0.001949554606365203\n",
            "[ 25 / 25 ] loss: 0.001949490851930394\n",
            "For Label 5:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.02953945824573238\n",
            "[ 2 / 25 ] loss: 0.02936525233364659\n",
            "[ 3 / 25 ] loss: 0.02945927668232711\n",
            "[ 4 / 25 ] loss: 0.02948706785846089\n",
            "[ 5 / 25 ] loss: 0.029519206886510864\n",
            "[ 6 / 25 ] loss: 0.029552469470911183\n",
            "[ 7 / 25 ] loss: 0.029555549285231467\n",
            "[ 8 / 25 ] loss: 0.029553776451605816\n",
            "[ 9 / 25 ] loss: 0.029546577345387067\n",
            "[ 10 / 25 ] loss: 0.029562448077989672\n",
            "[ 11 / 25 ] loss: 0.02957838040300308\n",
            "[ 12 / 25 ] loss: 0.02957370303497883\n",
            "[ 13 / 25 ] loss: 0.029561420375965833\n",
            "[ 14 / 25 ] loss: 0.029565393466799022\n",
            "[ 15 / 25 ] loss: 0.02956236663452318\n",
            "[ 16 / 25 ] loss: 0.029555443718913374\n",
            "[ 17 / 25 ] loss: 0.02955174075640412\n",
            "[ 18 / 25 ] loss: 0.029557209265773363\n",
            "[ 19 / 25 ] loss: 0.029547623478765377\n",
            "[ 20 / 25 ] loss: 0.02954669309975953\n",
            "[ 21 / 25 ] loss: 0.02954844954575331\n",
            "[ 22 / 25 ] loss: 0.029544319568057205\n",
            "[ 23 / 25 ] loss: 0.029540425648469808\n",
            "[ 24 / 25 ] loss: 0.02954684135108165\n",
            "[ 25 / 25 ] loss: 0.029558392799910144\n",
            "For Label 6:\n",
            "\n",
            "[ 1 / 25 ] loss: 0.006919107431528406\n",
            "[ 2 / 25 ] loss: 0.006362695162461872\n",
            "[ 3 / 25 ] loss: 0.006264491506024473\n",
            "[ 4 / 25 ] loss: 0.0062127814164646\n",
            "[ 5 / 25 ] loss: 0.006177398120213917\n",
            "[ 6 / 25 ] loss: 0.006152466390471916\n",
            "[ 7 / 25 ] loss: 0.00613582362620229\n",
            "[ 8 / 25 ] loss: 0.006124425470769931\n",
            "[ 9 / 25 ] loss: 0.006115726208572062\n",
            "[ 10 / 25 ] loss: 0.0061087915562951095\n",
            "[ 11 / 25 ] loss: 0.006103060844494014\n",
            "[ 12 / 25 ] loss: 0.006097672898035066\n",
            "[ 13 / 25 ] loss: 0.006091971724740936\n",
            "[ 14 / 25 ] loss: 0.006087139699188538\n",
            "[ 15 / 25 ] loss: 0.0060845706300100815\n",
            "[ 16 / 25 ] loss: 0.006083347444195653\n",
            "[ 17 / 25 ] loss: 0.0060819359242011125\n",
            "[ 18 / 25 ] loss: 0.006079470879419888\n",
            "[ 19 / 25 ] loss: 0.006075763248477252\n",
            "[ 20 / 25 ] loss: 0.006071001488383453\n",
            "[ 21 / 25 ] loss: 0.006065482036194248\n",
            "[ 22 / 25 ] loss: 0.006059452332738106\n",
            "[ 23 / 25 ] loss: 0.006053055229678713\n",
            "[ 24 / 25 ] loss: 0.006046361855950018\n",
            "[ 25 / 25 ] loss: 0.006039442106967917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#report accuracies on the test dataset \n",
        "result_bert = np.concatenate((l1_y_pred_bert, l2_y_pred_bert, l3_y_pred_bert, l4_y_pred_bert, l5_y_pred_bert, l6_y_pred_bert), axis=1)\n",
        "acc_bert = accuracy_score(result_bert, y_test_val)\n",
        "print(\"Accuracy :\", acc_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7_10dgQeau",
        "outputId": "c271b04f-e018-41ba-9c47-9893c20916d8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.8026040201319203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9BxRHWEH7CE"
      },
      "source": [
        "Now, compare your implemented MLP with the sklearn MLP layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-FZgOy8IL4z",
        "outputId": "423964c7-42a2-4a85-e0ee-7a1d1600e211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8789583919472319\n"
          ]
        }
      ],
      "source": [
        "#report accuracies both on train dataset and test dataset\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "bert_mlp = MLPClassifier(hidden_layer_sizes=(32,16,6), activation='relu',solver='adam', max_iter=200)\n",
        "bert_mlp.fit(bert_x_train, y_train_val)\n",
        "predictions = bert_mlp.predict(bert_x_test)\n",
        "print('Accuracy:', accuracy_score(y_test_val, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJdZaYCPYQK"
      },
      "source": [
        "Which MLP performed better? \n",
        "Understand and Analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLP I created is more accurate. The reason may be that I  used different activation functions at different levels. In Sklearn MLP, the activation function remained the same at all layers \n",
        " Since our MLP is just an MLP, sklearn's MLP performs better in certain environments. Sklearn MLP performs better with data that the  same activation was sufficient for all layers. \n",
        " As a result, SkLearn's MLP performance improved, but predictive measurements were inadequate."
      ],
      "metadata": {
        "id": "41ucuOVCYkB3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "2021201060_Assignment3_Q4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}